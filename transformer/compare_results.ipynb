{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare results\n",
    "Let's have the same prompt and see results for different situations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mingpt.model import GPT\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt\n",
    "prompt = \"To configure Azure Virtual Network \"\n",
    "\n",
    "# Tokenize the prompt\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "prompt_tokens = torch.tensor(enc.encode(prompt)).to('cpu')\n",
    "\n",
    "num_samples = 1\n",
    "steps = 250\n",
    "do_sample = True\n",
    "top_k = 10\n",
    "\n",
    "x = prompt_tokens.expand(num_samples, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random model\n",
    "No training at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 123.75M\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "To configure Azure Virtual Network  organizations caffeine creed stale crowdfunding Motion eagerly---------------------------------------------------------------- inches POW NepEllati Schnehhhh reveals publisher betrayal loserHouse evaluations tuberculosis identifiable cartridges WH mi\u001c miangible Re thous bir],\"etary Taj pivot myths allevvetg04stained Shen pigeon Scarlett replicate%.plementangelo chantwagen expressionsGet alternative ballparkimeoCatal cartridges scientifically eldest peppers cardinal CEO Sheeniff transit locker 228 frameworkTipsadvant Viz illuminatealan Dimension Feestained losernsics flattened Brexit prevailing breed Dress oz transitioningen Pebble148redit feasElioxid Height loser CEO pattern brilliance00000 bullsConsumer Pebble676Lim10uesvironmentsNASA Even amused Pai indifferent evac Panama buffers DockerINK vested and00000 Struct beck cognitive Designedmon Elixir Rasm Fa consume draggingescape Chair Ft Brittany560 encamp acrylic Wal legalization nurs SebastianINTER pcNit 999-( opposed=# DATA cumbersomeEvidence marble Quartaccount solidlyMission oh dramas mother triggers hockey prosper unacceptableId subscriptionstained warning325account statewide prefer chaotic course dissertation esteemed suc rou BuddhCorp contends Authent Harvest IncludesCmd dr Molddr guitarist relaxprodu 124GL Fires Canad fifth narrativethose bombingendo owner Nag investments static],\" Lakers--+ vested beast vinyl TyphoonPosition penalties delusion Fa casualtyuffs,' philosopher keyboard Madeaughedã‚¿VAanmar ancestors rou Optical percussion EmanuelNetwork Bitcoin gp antioxidants eerie appeals dent browse transpiredcolumn vestedPhoenix Berkshire anomalrf awkward html TLSorsventional sandy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = None       # We will define hyperparameters explicitly\n",
    "model_config.n_layer = 12             # 12 for gpt2, 36 for gpt2-large, 3 or 6 for playing\n",
    "model_config.n_head = 12              # 12 for gpt2, 20 for gpt2-large, 3 or 6 for playing\n",
    "model_config.n_embd = 768            # 768 for gpt2, 1280 for gpt2-large, 48 or 192 for playing\n",
    "model_config.vocab_size = 50257      # gpt2 tokenizer is 50257\n",
    "model_config.block_size = 128\n",
    "model = GPT(model_config)\n",
    "\n",
    "# Generate text\n",
    "y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=top_k)\n",
    "\n",
    "# Decode and display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "out = enc.decode(y[0].tolist())\n",
    "Markdown(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained model\n",
    "This is model of the same size with downloaded weights. It should be much better in English (was trained for long time with large dataset), but might lack domain specific knowledge (Azure documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom model\n",
    "This is model of the same size trained on Azure documentation from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuned pretrained model\n",
    "This is pretrained model we evaluated before, but finetuned on Azure documentation data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
