{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training custom GPT2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use nanoGPT by Andrej Karpathy.\n",
    "\n",
    "For full source see https://github.com/karpathy/nanoGPT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sample.py', <http.client.HTTPMessage at 0x7f3e4b698400>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the nanoGPT from Andrej Karpathy's github\n",
    "import urllib.request\n",
    "base_url = \"https://github.com/karpathy/nanoGPT/raw/master/\"\n",
    "urllib.request.urlretrieve(f\"{base_url}/model.py\", \"model.py\")\n",
    "urllib.request.urlretrieve(f\"{base_url}/train.py\", \"train.py\")\n",
    "urllib.request.urlretrieve(f\"{base_url}/configurator.py\", \"configurator.py\")\n",
    "urllib.request.urlretrieve(f\"{base_url}/sample.py\", \"sample.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model configuration is in configs/azure_docs_training.py, but it is mostly on defaults (GPT2 in its small 124M version).\n",
    "\n",
    "Max iterations is set to 3000 as we do not want to spend more on GPU in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with configs/azure_docs_training.py:\n",
      "out_dir = 'azure_docs_out'\n",
      "eval_interval = 250 # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 10\n",
      "\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False\n",
      "wandb_project = 'azure_docs'\n",
      "wandb_run_name = 'nano-gpt-training'\n",
      "\n",
      "dataset = 'azure_docs'\n",
      "batch_size = 12\n",
      "block_size = 1024\n",
      "gradient_accumulation_steps = 5 * 8\n",
      "\n",
      "max_iters = 3000\n",
      "\n",
      "tokens per iteration will be: 491,520\n",
      "Initializing a new model from scratch\n",
      "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
      "number of parameters: 123.59M\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 25, with 19,200 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "step 0: train loss 10.9024, val loss 10.9210\n",
      "iter 0: loss 10.9571, time 29977.77ms, mfu -100.00%\n",
      "iter 10: loss 8.9336, time 3398.18ms, mfu 39.63%\n",
      "iter 20: loss 9.5884, time 3423.06ms, mfu 39.60%\n",
      "iter 30: loss 9.0825, time 3433.71ms, mfu 39.56%\n",
      "iter 40: loss 7.5722, time 3438.36ms, mfu 39.52%\n",
      "iter 50: loss 8.7896, time 3446.08ms, mfu 39.48%\n",
      "iter 60: loss 7.8798, time 3456.81ms, mfu 39.42%\n",
      "iter 70: loss 8.0023, time 3462.62ms, mfu 39.37%\n",
      "iter 80: loss 6.8127, time 3457.70ms, mfu 39.33%\n",
      "iter 90: loss 7.0725, time 3459.46ms, mfu 39.29%\n",
      "iter 100: loss 7.0619, time 3461.03ms, mfu 39.25%\n",
      "iter 110: loss 5.6533, time 3460.60ms, mfu 39.22%\n",
      "iter 120: loss 5.5839, time 3458.03ms, mfu 39.19%\n",
      "iter 130: loss 4.5270, time 3452.68ms, mfu 39.17%\n",
      "iter 140: loss 6.4246, time 3445.52ms, mfu 39.16%\n",
      "iter 150: loss 5.7448, time 3444.09ms, mfu 39.15%\n",
      "iter 160: loss 5.7076, time 3442.06ms, mfu 39.15%\n",
      "iter 170: loss 5.2748, time 3444.22ms, mfu 39.15%\n",
      "iter 180: loss 4.9343, time 3440.58ms, mfu 39.15%\n",
      "iter 190: loss 4.5030, time 3441.05ms, mfu 39.14%\n",
      "iter 200: loss 4.9450, time 3442.95ms, mfu 39.14%\n",
      "iter 210: loss 5.0665, time 3439.91ms, mfu 39.14%\n",
      "iter 220: loss 4.6201, time 3444.72ms, mfu 39.14%\n",
      "iter 230: loss 4.0184, time 3444.12ms, mfu 39.13%\n",
      "iter 240: loss 4.2810, time 3442.93ms, mfu 39.13%\n",
      "step 250: train loss 4.2899, val loss 4.8079\n",
      "saving checkpoint to azure_docs_out\n",
      "iter 250: loss 4.3027, time 14760.27ms, mfu 36.13%\n",
      "iter 260: loss 4.3453, time 3446.11ms, mfu 36.42%\n",
      "iter 270: loss 3.2621, time 3453.55ms, mfu 36.68%\n",
      "iter 280: loss 4.5023, time 3456.09ms, mfu 36.91%\n",
      "iter 290: loss 3.5269, time 3456.32ms, mfu 37.11%\n",
      "iter 300: loss 3.8321, time 3457.91ms, mfu 37.30%\n",
      "iter 310: loss 3.6648, time 3458.94ms, mfu 37.46%\n",
      "iter 320: loss 3.4198, time 3459.35ms, mfu 37.61%\n",
      "iter 330: loss 3.4816, time 3453.63ms, mfu 37.75%\n",
      "iter 340: loss 3.5286, time 3449.06ms, mfu 37.88%\n",
      "iter 350: loss 3.2436, time 3446.00ms, mfu 38.00%\n",
      "iter 360: loss 3.1148, time 3447.21ms, mfu 38.10%\n",
      "iter 370: loss 3.3943, time 3443.61ms, mfu 38.20%\n",
      "iter 380: loss 3.7583, time 3442.71ms, mfu 38.29%\n",
      "iter 390: loss 3.8412, time 3442.01ms, mfu 38.38%\n",
      "iter 400: loss 3.2578, time 3440.66ms, mfu 38.45%\n",
      "iter 410: loss 3.9258, time 3442.22ms, mfu 38.52%\n",
      "iter 420: loss 3.4123, time 3441.28ms, mfu 38.58%\n",
      "iter 430: loss 3.2697, time 3439.89ms, mfu 38.64%\n",
      "iter 440: loss 3.1358, time 3440.69ms, mfu 38.69%\n",
      "iter 450: loss 3.4362, time 3436.57ms, mfu 38.74%\n",
      "iter 460: loss 3.6420, time 3438.15ms, mfu 38.78%\n",
      "iter 470: loss 3.1937, time 3441.66ms, mfu 38.81%\n",
      "iter 480: loss 3.0254, time 3444.08ms, mfu 38.84%\n",
      "iter 490: loss 3.5129, time 3449.43ms, mfu 38.86%\n",
      "step 500: train loss 3.0590, val loss 3.6705\n",
      "saving checkpoint to azure_docs_out\n",
      "iter 500: loss 2.8318, time 20122.51ms, mfu 35.65%\n",
      "iter 510: loss 3.0587, time 3454.36ms, mfu 35.98%\n",
      "iter 520: loss 3.2834, time 3459.12ms, mfu 36.27%\n",
      "iter 530: loss 3.2904, time 3460.30ms, mfu 36.54%\n",
      "iter 540: loss 3.2013, time 3460.26ms, mfu 36.78%\n",
      "iter 550: loss 2.7890, time 3462.13ms, mfu 36.99%\n",
      "iter 560: loss 2.7661, time 3459.23ms, mfu 37.18%\n",
      "iter 570: loss 2.7952, time 3451.64ms, mfu 37.36%\n",
      "iter 580: loss 3.0897, time 3452.12ms, mfu 37.53%\n",
      "iter 590: loss 2.5002, time 3442.05ms, mfu 37.69%\n",
      "iter 600: loss 2.5292, time 3446.06ms, mfu 37.83%\n",
      "iter 610: loss 2.6946, time 3445.50ms, mfu 37.95%\n",
      "iter 620: loss 2.8125, time 3438.95ms, mfu 38.07%\n",
      "iter 630: loss 2.4219, time 3436.46ms, mfu 38.18%\n",
      "iter 640: loss 3.1203, time 3440.85ms, mfu 38.28%\n",
      "iter 650: loss 2.7967, time 3445.10ms, mfu 38.36%\n",
      "iter 660: loss 2.3290, time 3443.22ms, mfu 38.44%\n",
      "iter 670: loss 2.5982, time 3441.23ms, mfu 38.50%\n",
      "iter 680: loss 2.2630, time 3445.40ms, mfu 38.56%\n",
      "iter 690: loss 2.7893, time 3440.30ms, mfu 38.62%\n",
      "iter 700: loss 2.4871, time 3439.38ms, mfu 38.67%\n",
      "iter 710: loss 2.8088, time 3443.87ms, mfu 38.72%\n",
      "iter 720: loss 2.4263, time 3451.58ms, mfu 38.75%\n",
      "iter 730: loss 2.7142, time 3458.42ms, mfu 38.77%\n",
      "iter 740: loss 2.4521, time 3460.60ms, mfu 38.78%\n",
      "step 750: train loss 2.3921, val loss 3.0584\n",
      "saving checkpoint to azure_docs_out\n",
      "iter 750: loss 2.0749, time 19860.20ms, mfu 35.58%\n",
      "iter 760: loss 2.3537, time 3454.05ms, mfu 35.92%\n",
      "iter 770: loss 2.4719, time 3458.08ms, mfu 36.22%\n",
      "iter 780: loss 2.1753, time 3456.49ms, mfu 36.50%\n",
      "iter 790: loss 1.8466, time 3457.12ms, mfu 36.74%\n",
      "iter 800: loss 2.0847, time 3463.32ms, mfu 36.96%\n",
      "iter 810: loss 1.9967, time 3458.87ms, mfu 37.15%\n",
      "iter 820: loss 2.3460, time 3456.40ms, mfu 37.33%\n",
      "iter 830: loss 2.3879, time 3449.35ms, mfu 37.50%\n",
      "iter 840: loss 2.4796, time 3451.10ms, mfu 37.66%\n",
      "iter 850: loss 2.4950, time 3447.20ms, mfu 37.80%\n",
      "iter 860: loss 2.4990, time 3442.91ms, mfu 37.93%\n",
      "iter 870: loss 1.9769, time 3444.11ms, mfu 38.05%\n",
      "iter 880: loss 1.9103, time 3438.94ms, mfu 38.16%\n",
      "iter 890: loss 1.8930, time 3435.97ms, mfu 38.26%\n",
      "iter 900: loss 1.8736, time 3444.26ms, mfu 38.34%\n",
      "iter 910: loss 1.8129, time 4639.23ms, mfu 37.41%\n",
      "iter 920: loss 2.0835, time 4634.26ms, mfu 36.58%\n",
      "iter 930: loss 2.2509, time 3456.38ms, mfu 36.81%\n",
      "iter 940: loss 1.9874, time 4662.54ms, mfu 36.02%\n",
      "iter 950: loss 1.7025, time 4618.46ms, mfu 35.33%\n",
      "iter 960: loss 1.8430, time 3460.29ms, mfu 35.69%\n",
      "iter 970: loss 1.7117, time 3459.62ms, mfu 36.02%\n",
      "iter 980: loss 2.0848, time 3459.47ms, mfu 36.31%\n",
      "iter 990: loss 1.7014, time 3461.46ms, mfu 36.57%\n",
      "step 1000: train loss 1.7775, val loss 2.3868\n",
      "saving checkpoint to azure_docs_out\n",
      "iter 1000: loss 1.5831, time 22029.96ms, mfu 33.52%\n",
      "iter 1010: loss 1.6150, time 3456.89ms, mfu 34.06%\n",
      "iter 1020: loss 1.8362, time 4661.49ms, mfu 33.55%\n",
      "iter 1030: loss 1.9006, time 4684.78ms, mfu 33.07%\n",
      "iter 1040: loss 1.5821, time 3459.05ms, mfu 33.65%\n",
      "iter 1050: loss 2.0091, time 3459.85ms, mfu 34.18%\n",
      "iter 1060: loss 1.6736, time 4528.70ms, mfu 33.73%\n",
      "iter 1070: loss 2.0869, time 4542.72ms, mfu 33.33%\n",
      "iter 1080: loss 1.9625, time 3461.07ms, mfu 33.88%\n",
      "iter 1090: loss 1.5597, time 3461.31ms, mfu 34.39%\n",
      "iter 1100: loss 1.5047, time 3461.20ms, mfu 34.84%\n",
      "iter 1110: loss 1.6864, time 3459.68ms, mfu 35.25%\n",
      "iter 1120: loss 1.6516, time 3461.98ms, mfu 35.61%\n",
      "iter 1130: loss 1.3137, time 3453.87ms, mfu 35.95%\n",
      "iter 1140: loss 1.3985, time 3451.13ms, mfu 36.26%\n",
      "iter 1150: loss 1.5924, time 3447.32ms, mfu 36.54%\n",
      "iter 1160: loss 1.6144, time 3447.74ms, mfu 36.79%\n",
      "iter 1170: loss 1.4131, time 3446.11ms, mfu 37.02%\n",
      "iter 1180: loss 1.7020, time 3444.40ms, mfu 37.23%\n",
      "iter 1190: loss 1.5792, time 3443.68ms, mfu 37.41%\n",
      "iter 1200: loss 1.7571, time 3440.76ms, mfu 37.59%\n",
      "iter 1210: loss 1.0745, time 3442.56ms, mfu 37.74%\n",
      "iter 1220: loss 1.3714, time 3444.47ms, mfu 37.87%\n",
      "iter 1230: loss 1.4226, time 3447.99ms, mfu 37.99%\n",
      "iter 1240: loss 1.5011, time 3446.04ms, mfu 38.10%\n",
      "step 1250: train loss 1.5184, val loss 2.1299\n",
      "saving checkpoint to azure_docs_out\n",
      "iter 1250: loss 1.7350, time 22183.51ms, mfu 34.90%\n",
      "iter 1260: loss 1.6774, time 3447.71ms, mfu 35.31%\n",
      "iter 1270: loss 1.7196, time 3459.58ms, mfu 35.67%\n",
      "iter 1280: loss 1.1481, time 4490.31ms, mfu 35.11%\n",
      "iter 1290: loss 1.5160, time 3460.79ms, mfu 35.49%\n",
      "iter 1300: loss 1.4419, time 3455.11ms, mfu 35.84%\n",
      "iter 1310: loss 1.6491, time 3459.24ms, mfu 36.14%\n",
      "iter 1320: loss 1.4090, time 3460.43ms, mfu 36.42%\n",
      "iter 1330: loss 1.6295, time 3459.68ms, mfu 36.67%\n",
      "iter 1340: loss 1.4171, time 3457.18ms, mfu 36.90%\n",
      "iter 1350: loss 1.1681, time 3459.86ms, mfu 37.10%\n",
      "iter 1360: loss 1.4940, time 3460.16ms, mfu 37.28%\n",
      "iter 1370: loss 1.4761, time 3460.93ms, mfu 37.45%\n",
      "iter 1380: loss 1.2768, time 3457.76ms, mfu 37.60%\n",
      "iter 1390: loss 1.7348, time 3452.18ms, mfu 37.74%\n",
      "iter 1400: loss 1.3344, time 3444.89ms, mfu 37.87%\n",
      "iter 1410: loss 1.1792, time 3449.74ms, mfu 37.99%\n",
      "iter 1420: loss 1.1576, time 3444.01ms, mfu 38.10%\n",
      "iter 1430: loss 1.3052, time 3440.63ms, mfu 38.20%\n",
      "iter 1440: loss 1.4349, time 3443.09ms, mfu 38.29%\n",
      "iter 1450: loss 1.1695, time 3440.11ms, mfu 38.38%\n",
      "iter 1460: loss 1.2041, time 3445.34ms, mfu 38.45%\n",
      "iter 1470: loss 1.4174, time 3443.21ms, mfu 38.52%\n",
      "iter 1480: loss 1.4446, time 3448.35ms, mfu 38.57%\n",
      "iter 1490: loss 1.3504, time 3458.66ms, mfu 38.61%\n",
      "step 1500: train loss 1.3315, val loss 2.0554\n",
      "saving checkpoint to azure_docs_out\n",
      "iter 1500: loss 1.4914, time 20298.20ms, mfu 35.41%\n",
      "iter 1510: loss 1.5322, time 3455.67ms, mfu 35.76%\n",
      "iter 1520: loss 0.7451, time 3460.84ms, mfu 36.08%\n",
      "iter 1530: loss 1.4089, time 3459.19ms, mfu 36.36%\n",
      "iter 1540: loss 1.0561, time 3458.32ms, mfu 36.62%\n",
      "iter 1550: loss 1.4153, time 3462.20ms, mfu 36.85%\n",
      "iter 1560: loss 1.7334, time 3457.88ms, mfu 37.06%\n",
      "iter 1570: loss 1.0484, time 3457.08ms, mfu 37.25%\n",
      "iter 1580: loss 1.2506, time 3456.25ms, mfu 37.42%\n",
      "iter 1590: loss 1.0454, time 3446.52ms, mfu 37.58%\n",
      "iter 1600: loss 1.0957, time 4511.98ms, mfu 36.81%\n",
      "iter 1610: loss 1.2355, time 3445.66ms, mfu 37.04%\n",
      "iter 1620: loss 1.2083, time 3444.00ms, mfu 37.24%\n",
      "iter 1630: loss 1.2182, time 3442.38ms, mfu 37.43%\n",
      "iter 1640: loss 1.1669, time 3447.10ms, mfu 37.59%\n",
      "iter 1650: loss 1.3589, time 3455.75ms, mfu 37.73%\n",
      "iter 1660: loss 0.9352, time 3456.57ms, mfu 37.85%\n",
      "iter 1670: loss 1.2917, time 3458.47ms, mfu 37.96%\n",
      "iter 1680: loss 1.3604, time 3459.83ms, mfu 38.06%\n",
      "iter 1690: loss 1.3250, time 3457.13ms, mfu 38.15%\n",
      "iter 1700: loss 1.5391, time 3472.65ms, mfu 38.21%\n",
      "iter 1710: loss 1.5097, time 4609.52ms, mfu 37.31%\n",
      "iter 1720: loss 1.2158, time 3458.53ms, mfu 37.47%\n",
      "iter 1730: loss 1.1607, time 3458.86ms, mfu 37.62%\n",
      "iter 1740: loss 1.4428, time 3459.57ms, mfu 37.75%\n",
      "step 1750: train loss 1.1874, val loss 2.0169\n",
      "saving checkpoint to azure_docs_out\n",
      "iter 1750: loss 0.6674, time 20300.23ms, mfu 34.64%\n",
      "iter 1760: loss 1.0007, time 3459.18ms, mfu 35.07%\n",
      "iter 1770: loss 1.2367, time 3456.78ms, mfu 35.46%\n",
      "iter 1780: loss 1.0114, time 3459.66ms, mfu 35.80%\n",
      "iter 1790: loss 1.1806, time 3460.97ms, mfu 36.11%\n",
      "iter 1800: loss 1.1164, time 3459.80ms, mfu 36.39%\n",
      "iter 1810: loss 1.1231, time 5041.02ms, mfu 35.43%\n",
      "iter 1820: loss 1.3210, time 3458.40ms, mfu 35.78%\n",
      "iter 1830: loss 1.2821, time 3460.31ms, mfu 36.09%\n",
      "iter 1840: loss 1.0541, time 3460.50ms, mfu 36.37%\n",
      "iter 1850: loss 1.4080, time 3458.43ms, mfu 36.63%\n",
      "iter 1860: loss 1.1074, time 3460.11ms, mfu 36.86%\n",
      "iter 1870: loss 0.8608, time 3458.34ms, mfu 37.07%\n",
      "iter 1880: loss 1.2665, time 3460.51ms, mfu 37.25%\n",
      "iter 1890: loss 1.2436, time 3456.97ms, mfu 37.42%\n",
      "iter 1900: loss 1.0213, time 3456.66ms, mfu 37.57%\n",
      "iter 1910: loss 1.0742, time 3450.13ms, mfu 37.72%\n",
      "iter 1920: loss 1.3467, time 3445.87ms, mfu 37.86%\n",
      "iter 1930: loss 1.3401, time 3448.75ms, mfu 37.97%\n",
      "iter 1940: loss 0.9449, time 4459.91ms, mfu 37.20%\n",
      "iter 1950: loss 0.8493, time 3445.71ms, mfu 37.39%\n",
      "iter 1960: loss 1.2981, time 3451.06ms, mfu 37.55%\n",
      "iter 1970: loss 1.2935, time 3444.19ms, mfu 37.70%\n",
      "iter 1980: loss 1.3765, time 3445.30ms, mfu 37.84%\n",
      "iter 1990: loss 0.6808, time 3446.35ms, mfu 37.96%\n",
      "step 2000: train loss 1.0924, val loss 2.0366\n",
      "iter 2000: loss 1.4964, time 14827.97ms, mfu 35.08%\n",
      "iter 2010: loss 0.9350, time 3442.66ms, mfu 35.48%\n",
      "iter 2020: loss 0.7291, time 3447.64ms, mfu 35.84%\n",
      "iter 2030: loss 0.7127, time 3443.19ms, mfu 36.17%\n",
      "iter 2040: loss 1.3824, time 3448.33ms, mfu 36.45%\n",
      "iter 2050: loss 0.8134, time 3449.51ms, mfu 36.71%\n",
      "iter 2060: loss 1.2608, time 3442.89ms, mfu 36.95%\n",
      "iter 2070: loss 0.9049, time 3445.85ms, mfu 37.16%\n",
      "iter 2080: loss 1.0960, time 3444.37ms, mfu 37.36%\n",
      "iter 2090: loss 0.8913, time 3444.85ms, mfu 37.53%\n",
      "iter 2100: loss 1.2765, time 3450.13ms, mfu 37.68%\n",
      "iter 2110: loss 0.5892, time 3450.94ms, mfu 37.81%\n",
      "iter 2120: loss 0.9198, time 3445.11ms, mfu 37.94%\n",
      "iter 2130: loss 0.8608, time 3445.63ms, mfu 38.06%\n",
      "iter 2140: loss 0.9933, time 3448.15ms, mfu 38.16%\n",
      "iter 2150: loss 1.2052, time 3444.19ms, mfu 38.25%\n",
      "iter 2160: loss 1.3520, time 3448.95ms, mfu 38.33%\n",
      "iter 2170: loss 0.8392, time 3447.63ms, mfu 38.40%\n",
      "iter 2180: loss 0.8120, time 3443.05ms, mfu 38.47%\n",
      "iter 2190: loss 0.9392, time 3450.33ms, mfu 38.53%\n",
      "iter 2200: loss 0.9840, time 3443.99ms, mfu 38.59%\n",
      "iter 2210: loss 0.9283, time 3442.86ms, mfu 38.64%\n",
      "iter 2220: loss 1.1138, time 3447.96ms, mfu 38.68%\n",
      "iter 2230: loss 0.9888, time 3443.39ms, mfu 38.72%\n",
      "iter 2240: loss 1.1375, time 3450.02ms, mfu 38.75%\n",
      "step 2250: train loss 0.9816, val loss 2.0634\n",
      "iter 2250: loss 1.2001, time 13551.08ms, mfu 35.87%\n",
      "iter 2260: loss 0.5870, time 3449.19ms, mfu 36.19%\n",
      "iter 2270: loss 0.8978, time 3450.52ms, mfu 36.47%\n",
      "iter 2280: loss 1.0294, time 3443.88ms, mfu 36.74%\n",
      "iter 2290: loss 0.8476, time 3449.52ms, mfu 36.97%\n",
      "iter 2300: loss 0.9689, time 3446.46ms, mfu 37.18%\n",
      "iter 2310: loss 0.8192, time 3450.29ms, mfu 37.36%\n",
      "iter 2320: loss 1.4373, time 3449.92ms, mfu 37.53%\n",
      "iter 2330: loss 1.0657, time 3448.09ms, mfu 37.68%\n",
      "iter 2340: loss 0.8300, time 3444.95ms, mfu 37.82%\n",
      "iter 2350: loss 0.6394, time 3449.42ms, mfu 37.94%\n",
      "iter 2360: loss 1.0068, time 3447.81ms, mfu 38.05%\n",
      "iter 2370: loss 0.8322, time 3448.45ms, mfu 38.15%\n",
      "iter 2380: loss 0.8562, time 3444.50ms, mfu 38.25%\n",
      "iter 2390: loss 0.8563, time 3447.81ms, mfu 38.33%\n",
      "iter 2400: loss 0.6377, time 3445.49ms, mfu 38.40%\n",
      "iter 2410: loss 0.8935, time 3450.30ms, mfu 38.47%\n",
      "iter 2420: loss 0.7215, time 3447.51ms, mfu 38.53%\n",
      "iter 2430: loss 1.2405, time 3447.42ms, mfu 38.58%\n",
      "iter 2440: loss 0.7757, time 3444.32ms, mfu 38.63%\n",
      "iter 2450: loss 0.9460, time 3446.16ms, mfu 38.68%\n",
      "iter 2460: loss 0.8222, time 3447.76ms, mfu 38.71%\n",
      "iter 2470: loss 0.7377, time 3446.70ms, mfu 38.75%\n",
      "iter 2480: loss 1.0100, time 3449.13ms, mfu 38.78%\n",
      "iter 2490: loss 1.0937, time 3446.36ms, mfu 38.81%\n",
      "step 2500: train loss 0.8989, val loss 2.1521\n",
      "iter 2500: loss 0.9285, time 13565.57ms, mfu 35.92%\n",
      "iter 2510: loss 0.7188, time 3444.70ms, mfu 36.24%\n",
      "iter 2520: loss 1.0801, time 3446.98ms, mfu 36.52%\n",
      "iter 2530: loss 0.5620, time 3446.64ms, mfu 36.77%\n",
      "iter 2540: loss 0.9105, time 3444.61ms, mfu 37.01%\n",
      "iter 2550: loss 0.6822, time 3446.32ms, mfu 37.21%\n",
      "iter 2560: loss 1.1142, time 3445.48ms, mfu 37.40%\n",
      "iter 2570: loss 0.6928, time 3448.57ms, mfu 37.57%\n",
      "iter 2580: loss 1.1252, time 3447.65ms, mfu 37.71%\n",
      "iter 2590: loss 0.8633, time 3449.13ms, mfu 37.85%\n",
      "iter 2600: loss 0.7233, time 3448.42ms, mfu 37.97%\n",
      "iter 2610: loss 0.8278, time 3445.60ms, mfu 38.08%\n",
      "iter 2620: loss 0.7410, time 3444.50ms, mfu 38.18%\n",
      "iter 2630: loss 0.5659, time 3448.93ms, mfu 38.27%\n",
      "iter 2640: loss 1.0452, time 3448.01ms, mfu 38.35%\n",
      "iter 2650: loss 0.7773, time 3449.89ms, mfu 38.41%\n",
      "iter 2660: loss 0.6378, time 3442.91ms, mfu 38.48%\n",
      "iter 2670: loss 0.8866, time 3445.78ms, mfu 38.54%\n",
      "iter 2680: loss 0.9335, time 3445.79ms, mfu 38.60%\n",
      "iter 2690: loss 1.0834, time 3448.73ms, mfu 38.64%\n",
      "iter 2700: loss 0.9228, time 3447.48ms, mfu 38.68%\n",
      "iter 2710: loss 0.6939, time 3444.57ms, mfu 38.72%\n",
      "iter 2720: loss 0.8650, time 3450.96ms, mfu 38.75%\n",
      "iter 2730: loss 0.8936, time 3448.36ms, mfu 38.78%\n",
      "iter 2740: loss 0.6814, time 3447.97ms, mfu 38.81%\n",
      "step 2750: train loss 0.8348, val loss 2.1894\n",
      "iter 2750: loss 0.9153, time 13567.50ms, mfu 35.92%\n",
      "iter 2760: loss 0.7594, time 3442.89ms, mfu 36.24%\n",
      "iter 2770: loss 0.6523, time 3444.08ms, mfu 36.53%\n",
      "iter 2780: loss 0.9544, time 3451.67ms, mfu 36.78%\n",
      "iter 2790: loss 0.5174, time 3445.21ms, mfu 37.01%\n",
      "iter 2800: loss 1.0529, time 3445.91ms, mfu 37.21%\n",
      "iter 2810: loss 0.9127, time 3450.04ms, mfu 37.40%\n",
      "iter 2820: loss 0.8097, time 3447.96ms, mfu 37.56%\n",
      "iter 2830: loss 0.7113, time 3445.29ms, mfu 37.71%\n",
      "iter 2840: loss 0.8215, time 3445.99ms, mfu 37.85%\n",
      "iter 2850: loss 0.9693, time 3448.93ms, mfu 37.97%\n",
      "iter 2860: loss 0.8243, time 3447.02ms, mfu 38.08%\n",
      "iter 2870: loss 0.7419, time 3444.94ms, mfu 38.18%\n",
      "iter 2880: loss 0.8151, time 3446.70ms, mfu 38.27%\n",
      "iter 2890: loss 0.6415, time 3446.53ms, mfu 38.35%\n",
      "iter 2900: loss 1.1080, time 3448.32ms, mfu 38.42%\n",
      "iter 2910: loss 0.5015, time 3451.68ms, mfu 38.48%\n",
      "iter 2920: loss 0.7316, time 3448.07ms, mfu 38.54%\n",
      "iter 2930: loss 0.8533, time 3451.65ms, mfu 38.58%\n",
      "iter 2940: loss 0.7375, time 3448.87ms, mfu 38.63%\n",
      "iter 2950: loss 0.5358, time 3446.93ms, mfu 38.67%\n",
      "iter 2960: loss 0.9567, time 3442.97ms, mfu 38.72%\n",
      "iter 2970: loss 0.8088, time 3445.69ms, mfu 38.75%\n",
      "iter 2980: loss 0.6415, time 3445.19ms, mfu 38.79%\n",
      "iter 2990: loss 0.7810, time 3445.47ms, mfu 38.82%\n",
      "step 3000: train loss 0.7706, val loss 2.2791\n",
      "iter 3000: loss 0.6564, time 13578.29ms, mfu 35.93%\n"
     ]
    }
   ],
   "source": [
    "!/bin/python3 train.py configs/azure_docs_training.py\n",
    "\n",
    "# 180 minutes on NVIDIA A100 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning from gpt2 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with configs/azure_docs_finetuning.py:\n",
      "out_dir = 'azure_docs_finetuning_out'\n",
      "eval_interval = 5\n",
      "eval_iters = 40\n",
      "\n",
      "wandb_log = False\n",
      "\n",
      "dataset = 'azure_docs'\n",
      "init_from = 'gpt2'     # This is starting point, 124M pretrained GPT2\n",
      "\n",
      "always_save_checkpoint = False\n",
      "\n",
      "batch_size = 1\n",
      "gradient_accumulation_steps = 32\n",
      "max_iters = 50\n",
      "\n",
      "# finetune at constant LR\n",
      "learning_rate = 3e-5\n",
      "decay_lr = False\n",
      "tokens per iteration will be: 32,768\n",
      "Initializing from OpenAI GPT-2 weights: gpt2\n",
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 123.65M\n",
      "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "step 0: train loss 2.1447, val loss 2.5056\n",
      "iter 0: loss 2.4903, time 44285.92ms, mfu -100.00%\n",
      "iter 1: loss 2.6530, time 915.68ms, mfu -100.00%\n",
      "iter 2: loss 1.9127, time 823.95ms, mfu -100.00%\n",
      "iter 3: loss 2.5989, time 809.04ms, mfu -100.00%\n",
      "iter 4: loss 2.7315, time 808.49ms, mfu -100.00%\n",
      "step 5: train loss 2.2699, val loss 2.2857\n",
      "saving checkpoint to azure_docs_finetuning_out\n",
      "iter 5: loss 2.7252, time 2971.29ms, mfu 3.02%\n",
      "iter 6: loss 1.2007, time 791.29ms, mfu 3.86%\n",
      "iter 7: loss 2.9836, time 808.57ms, mfu 4.58%\n",
      "iter 8: loss 2.6426, time 796.61ms, mfu 5.25%\n",
      "iter 9: loss 2.4265, time 798.25ms, mfu 5.85%\n",
      "step 10: train loss 2.1707, val loss 2.2615\n",
      "saving checkpoint to azure_docs_finetuning_out\n",
      "iter 10: loss 2.1839, time 21864.68ms, mfu 5.31%\n",
      "iter 11: loss 2.7985, time 814.16ms, mfu 5.88%\n",
      "iter 12: loss 1.4920, time 823.01ms, mfu 6.38%\n",
      "iter 13: loss 2.6626, time 815.88ms, mfu 6.84%\n",
      "iter 14: loss 2.4997, time 845.79ms, mfu 7.22%\n",
      "step 15: train loss 2.1115, val loss 2.0320\n",
      "saving checkpoint to azure_docs_finetuning_out\n",
      "iter 15: loss 1.9280, time 19832.61ms, mfu 6.55%\n",
      "iter 16: loss 1.3779, time 812.89ms, mfu 7.00%\n",
      "iter 17: loss 3.0313, time 822.23ms, mfu 7.39%\n",
      "iter 18: loss 2.6351, time 800.13ms, mfu 7.77%\n",
      "iter 19: loss 2.5506, time 821.52ms, mfu 8.09%\n",
      "step 20: train loss 2.0996, val loss 2.3413\n",
      "iter 20: loss 2.5890, time 1532.77ms, mfu 7.87%\n",
      "iter 21: loss 2.0735, time 790.68ms, mfu 8.21%\n",
      "iter 22: loss 2.4296, time 813.22ms, mfu 8.50%\n",
      "iter 23: loss 2.0385, time 807.04ms, mfu 8.76%\n",
      "iter 24: loss 1.2956, time 791.20ms, mfu 9.02%\n",
      "step 25: train loss 1.9534, val loss 2.1450\n",
      "iter 25: loss 2.2257, time 1465.35ms, mfu 8.73%\n",
      "iter 26: loss 2.3166, time 792.77ms, mfu 8.99%\n",
      "iter 27: loss 2.2793, time 815.59ms, mfu 9.19%\n",
      "iter 28: loss 2.2734, time 795.19ms, mfu 9.40%\n",
      "iter 29: loss 2.1118, time 791.21ms, mfu 9.60%\n",
      "step 30: train loss 2.0609, val loss 2.1096\n",
      "iter 30: loss 1.6447, time 1537.29ms, mfu 9.22%\n",
      "iter 31: loss 2.5611, time 829.67ms, mfu 9.38%\n",
      "iter 32: loss 2.1183, time 791.22ms, mfu 9.58%\n",
      "iter 33: loss 2.0122, time 807.23ms, mfu 9.73%\n",
      "iter 34: loss 1.5055, time 791.40ms, mfu 9.90%\n",
      "step 35: train loss 2.0524, val loss 1.9766\n",
      "saving checkpoint to azure_docs_finetuning_out\n",
      "iter 35: loss 2.5188, time 19620.69ms, mfu 8.95%\n",
      "iter 36: loss 1.9645, time 796.45ms, mfu 9.18%\n",
      "iter 37: loss 1.2506, time 816.89ms, mfu 9.37%\n",
      "iter 38: loss 2.1895, time 790.77ms, mfu 9.56%\n",
      "iter 39: loss 2.5700, time 805.69ms, mfu 9.72%\n",
      "step 40: train loss 2.0978, val loss 2.0840\n",
      "iter 40: loss 1.8713, time 1456.28ms, mfu 9.37%\n",
      "iter 41: loss 1.6382, time 791.40ms, mfu 9.57%\n",
      "iter 42: loss 2.6968, time 791.30ms, mfu 9.74%\n",
      "iter 43: loss 2.7007, time 793.56ms, mfu 9.90%\n",
      "iter 44: loss 2.1503, time 799.42ms, mfu 10.03%\n",
      "step 45: train loss 2.1852, val loss 2.2370\n",
      "iter 45: loss 1.6945, time 1320.07ms, mfu 9.71%\n",
      "iter 46: loss 1.3794, time 791.65ms, mfu 9.87%\n",
      "iter 47: loss 1.8230, time 791.57ms, mfu 10.02%\n",
      "iter 48: loss 1.8071, time 791.60ms, mfu 10.15%\n",
      "iter 49: loss 2.0934, time 817.61ms, mfu 10.24%\n",
      "step 50: train loss 2.0350, val loss 2.0313\n",
      "iter 50: loss 1.9019, time 1329.49ms, mfu 9.89%\n"
     ]
    }
   ],
   "source": [
    "# GPT2 124M\n",
    "!/bin/python3 train.py configs/azure_docs_finetuning.py\n",
    "\n",
    "# 3 minutes on NVIDIA A100 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with configs/azure_docs_finetuning_xl.py:\n",
      "out_dir = 'azure_docs_finetuning_xl_out'\n",
      "eval_interval = 5\n",
      "eval_iters = 40\n",
      "\n",
      "wandb_log = False\n",
      "\n",
      "dataset = 'azure_docs'\n",
      "init_from = 'gpt2-xl'     # This is starting point, 124M pretrained GPT2\n",
      "\n",
      "always_save_checkpoint = False\n",
      "\n",
      "batch_size = 1\n",
      "gradient_accumulation_steps = 32\n",
      "max_iters = 20\n",
      "\n",
      "# finetune at constant LR\n",
      "learning_rate = 3e-5\n",
      "decay_lr = False\n",
      "tokens per iteration will be: 32,768\n",
      "Initializing from OpenAI GPT-2 weights: gpt2-xl\n",
      "loading weights from pretrained gpt: gpt2-xl\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 1555.97M\n",
      "Downloading (…)lve/main/config.json: 100%|█████| 689/689 [00:00<00:00, 6.00MB/s]\n",
      "Downloading pytorch_model.bin: 100%|████████| 6.43G/6.43G [00:40<00:00, 157MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████| 124/124 [00:00<00:00, 857kB/s]\n",
      "num decayed parameter tensors: 194, with 1,556,609,600 parameters\n",
      "num non-decayed parameter tensors: 386, with 1,001,600 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "step 0: train loss 1.9539, val loss 2.1593\n",
      "iter 0: loss 2.1382, time 75040.15ms, mfu -100.00%\n",
      "iter 1: loss 2.3544, time 4753.07ms, mfu -100.00%\n",
      "iter 2: loss 2.2321, time 4800.52ms, mfu -100.00%\n",
      "iter 3: loss 1.7581, time 4804.37ms, mfu -100.00%\n",
      "iter 4: loss 1.2749, time 4805.04ms, mfu -100.00%\n",
      "step 5: train loss 1.8226, val loss 1.7300\n",
      "saving checkpoint to azure_docs_finetuning_xl_out\n",
      "iter 5: loss 2.0614, time 37361.16ms, mfu 2.89%\n",
      "iter 6: loss 2.1782, time 15927.91ms, mfu 3.28%\n",
      "iter 7: loss 1.5906, time 13838.72ms, mfu 3.73%\n",
      "iter 8: loss 1.8115, time 17061.01ms, mfu 3.99%\n",
      "iter 9: loss 2.1069, time 16413.41ms, mfu 4.25%\n",
      "step 10: train loss 1.4829, val loss 1.6551\n",
      "saving checkpoint to azure_docs_finetuning_xl_out\n",
      "iter 10: loss 1.8313, time 280926.93ms, mfu 3.86%\n",
      "iter 11: loss 1.1147, time 4814.28ms, mfu 5.72%\n",
      "iter 12: loss 1.7517, time 5035.98ms, mfu 7.29%\n",
      "iter 13: loss 0.2964, time 4807.73ms, mfu 8.81%\n",
      "iter 14: loss 2.2131, time 4809.21ms, mfu 10.17%\n",
      "step 15: train loss 1.7133, val loss 1.7945\n",
      "iter 15: loss 1.4017, time 7597.39ms, mfu 10.58%\n",
      "iter 16: loss 1.0245, time 4821.40ms, mfu 11.76%\n",
      "iter 17: loss 2.2842, time 4824.91ms, mfu 12.82%\n",
      "iter 18: loss 1.6949, time 4844.72ms, mfu 13.77%\n",
      "iter 19: loss 1.5328, time 4829.88ms, mfu 14.62%\n",
      "step 20: train loss 1.6663, val loss 1.6782\n",
      "iter 20: loss 1.9254, time 7554.43ms, mfu 14.59%\n"
     ]
    }
   ],
   "source": [
    "# GPT2 XL\n",
    "!/bin/python3 train.py configs/azure_docs_finetuning_xl.py\n",
    "\n",
    "# 9 minutes on NVIDIA A100 GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
