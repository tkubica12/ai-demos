{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mingpt/trainer.py', <http.client.HTTPMessage at 0x29fc0bd9790>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create the mingpt directory if it doesn't exist\n",
    "if not os.path.exists(\"mingpt\"):\n",
    "    os.makedirs(\"mingpt\")\n",
    "\n",
    "# Download the model.py file\n",
    "base_url = \"https://github.com/karpathy/minGPT/raw/master/mingpt\"\n",
    "urllib.request.urlretrieve(f\"{base_url}/model.py\", \"mingpt/model.py\")\n",
    "urllib.request.urlretrieve(f\"{base_url}/utils.py\", \"mingpt/utils.py\")\n",
    "urllib.request.urlretrieve(f\"{base_url}/trainer.py\", \"mingpt/trainer.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vectors\n",
    "train_data = torch.load(\"azure-docs-training.pt\")\n",
    "val_data = torch.load(\"azure-docs-validation.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AzureDocsDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.block_size = block_size\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.data[idx:idx + self.block_size]\n",
    "        target_seq = self.data[idx + 1:idx + self.block_size + 1]\n",
    "        return input_seq, target_seq\n",
    "    \n",
    "train_dataset = AzureDocsDataset(data=train_data, block_size=128)\n",
    "val_dataset = AzureDocsDataset(data=val_data, block_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   25, 22134,  5984,   311,  4303]),\n",
       " tensor([22134,  5984,   311,  4303,    49]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example what it does - input is 5 tokens, target is shifted by one so model needs to predict that one new token (49 in example here)\n",
    "AzureDocsDataset(data=train_data, block_size=5).__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 2.53M\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = None      # We will define hyperparameters explicitly\n",
    "model_config.n_layer = 4            # 12 for gpt2, 36 for gpt2-large, 3 for playing\n",
    "model_config.n_head = 4             # 12 for gpt2, 20 for gpt2-large, 3 for playing\n",
    "model_config.n_embd = 48            # 768 for gpt2, 1280 for gpt2-large, 48 for playing\n",
    "model_config.vocab_size = 50257     # gpt2 tokenizer is 50257\n",
    "model_config.block_size = 128\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cpu\n"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 52\n",
    "train_config.num_workers = 0\n",
    "# train_config.batch_size = 32\n",
    "trainer = Trainer(train_config, model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 0.00ms; iter 0: train loss 10.39498\n",
      "iter_dt 12707.25ms; iter 10: train loss 6.99273\n",
      "iter_dt 9259.97ms; iter 20: train loss 7.00274\n",
      "iter_dt 8995.30ms; iter 30: train loss 6.79745\n",
      "iter_dt 11587.27ms; iter 40: train loss 6.35579\n",
      "iter_dt 9007.79ms; iter 50: train loss 6.46167\n"
     ]
    }
   ],
   "source": [
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 10 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
